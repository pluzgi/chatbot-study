{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Phase 6: Exploratory Analysis\n",
    "\n",
    "**Swiss Ballot Chatbot Study - Measurement Analysis**\n",
    "\n",
    "2x2 Factorial Design: Transparency (T0/T1) x Control (C0/C1)\n",
    "\n",
    "## Purpose\n",
    "\n",
    "### 6A. Dashboard Behavior (C1 only: Conditions C & D)\n",
    "- Frequency analysis of each dashboard variable (scope/purpose/storage/retention)\n",
    "- Compare C vs D distributions for each dashboard variable (χ² + Cramér's V)\n",
    "- Optional: cluster analysis of dashboard preference profiles\n",
    "\n",
    "### 6B. Q14 Open Text (\"What mattered most…\")\n",
    "- Theme codebook (multi-label coding)\n",
    "- Theme frequencies by condition (A/B/C/D)\n",
    "- Theme frequencies by donate vs decline\n",
    "- Condition contrasts: A vs B, A vs C, C vs D, B vs D\n",
    "- 5 short representative quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Import from Phase 1\n",
    "from phase1_descriptive_statistics import (\n",
    "    AnalysisConfig,\n",
    "    load_participant_data,\n",
    "    prepare_variables,\n",
    "    compute_sample_flow\n",
    ")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Significance threshold\n",
    "ALPHA = 0.05\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set participant type: 'ai' for AI test users, 'human' for real participants\n",
    "PARTICIPANT_TYPE = 'ai'\n",
    "\n",
    "config = AnalysisConfig(is_ai_participant=(PARTICIPANT_TYPE == 'ai'))\n",
    "print(f\"Analyzing: {'AI Test Users' if PARTICIPANT_TYPE == 'ai' else 'Human Participants'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(contingency_table: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Calculate Cramér's V effect size for a contingency table.\n",
    "    \n",
    "    Returns: (V, interpretation)\n",
    "    Interpretation: V < 0.1 = negligible, 0.1-0.2 = small, 0.2-0.4 = medium, >= 0.4 = large\n",
    "    \"\"\"\n",
    "    chi2 = stats.chi2_contingency(contingency_table)[0]\n",
    "    n = contingency_table.sum()\n",
    "    min_dim = min(contingency_table.shape) - 1\n",
    "    \n",
    "    V = np.sqrt(chi2 / (n * min_dim)) if (n * min_dim) > 0 else 0\n",
    "    \n",
    "    # Interpretation\n",
    "    if V < 0.1:\n",
    "        interpretation = \"negligible\"\n",
    "    elif V < 0.2:\n",
    "        interpretation = \"small\"\n",
    "    elif V < 0.4:\n",
    "        interpretation = \"medium\"\n",
    "    else:\n",
    "        interpretation = \"large\"\n",
    "    \n",
    "    return V, interpretation\n",
    "\n",
    "\n",
    "def chi_square_test(data: pd.DataFrame, var1: str, var2: str) -> dict:\n",
    "    \"\"\"\n",
    "    Perform chi-square test between two categorical variables.\n",
    "    \n",
    "    Returns dict with contingency table, chi2, df, p, Cramér's V.\n",
    "    \"\"\"\n",
    "    ct = pd.crosstab(data[var1], data[var2])\n",
    "    chi2, p, df, expected = stats.chi2_contingency(ct)\n",
    "    V, V_interp = cramers_v(ct.values)\n",
    "    \n",
    "    return {\n",
    "        'contingency_table': ct,\n",
    "        'chi2': chi2,\n",
    "        'df': df,\n",
    "        'p': p,\n",
    "        'cramers_v': V,\n",
    "        'v_interpretation': V_interp,\n",
    "        'expected': expected\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_raw = load_participant_data(config)\n",
    "df = prepare_variables(df_raw, config)\n",
    "\n",
    "# Apply exclusions\n",
    "sample_flow = compute_sample_flow(df)\n",
    "df_filtered = sample_flow['df_filtered']\n",
    "\n",
    "print(f\"\\nFinal sample size: N = {len(df_filtered)}\")\n",
    "\n",
    "# Filter to C1 conditions (C and D) for dashboard analysis\n",
    "df_c1 = df_filtered[df_filtered['control_level'] == 1].copy()\n",
    "print(f\"C1 participants (Conditions C & D): n = {len(df_c1)}\")\n",
    "print(f\"  Condition C: n = {len(df_c1[df_c1['condition'] == 'C'])}\")\n",
    "print(f\"  Condition D: n = {len(df_c1[df_c1['condition'] == 'D'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 6A: Dashboard Behavior Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"6A: DASHBOARD BEHAVIOR ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nAnalyzing dashboard selections for Conditions C & D only.\")\n",
    "\n",
    "# Dashboard variables\n",
    "dashboard_vars = ['dashboard_scope', 'dashboard_purpose', 'dashboard_storage', 'dashboard_retention']\n",
    "\n",
    "# Check which variables exist\n",
    "available_vars = [v for v in dashboard_vars if v in df_c1.columns]\n",
    "print(f\"\\nAvailable dashboard variables: {available_vars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency tables for each dashboard variable\n",
    "print(\"\\nDASHBOARD OPTION FREQUENCIES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dashboard_freq_results = {}\n",
    "\n",
    "for var in available_vars:\n",
    "    print(f\"\\n--- {var.upper()} ---\")\n",
    "    \n",
    "    # Overall frequencies\n",
    "    freq_overall = df_c1[var].value_counts().sort_index()\n",
    "    pct_overall = (df_c1[var].value_counts(normalize=True) * 100).sort_index()\n",
    "    \n",
    "    # By condition (C vs D)\n",
    "    freq_c = df_c1[df_c1['condition'] == 'C'][var].value_counts().sort_index()\n",
    "    freq_d = df_c1[df_c1['condition'] == 'D'][var].value_counts().sort_index()\n",
    "    \n",
    "    pct_c = (df_c1[df_c1['condition'] == 'C'][var].value_counts(normalize=True) * 100).sort_index()\n",
    "    pct_d = (df_c1[df_c1['condition'] == 'D'][var].value_counts(normalize=True) * 100).sort_index()\n",
    "    \n",
    "    # Combine into table\n",
    "    freq_table = pd.DataFrame({\n",
    "        'Overall n': freq_overall,\n",
    "        'Overall %': pct_overall.round(1),\n",
    "        'C n': freq_c,\n",
    "        'C %': pct_c.round(1),\n",
    "        'D n': freq_d,\n",
    "        'D %': pct_d.round(1)\n",
    "    }).fillna(0)\n",
    "    \n",
    "    print(freq_table)\n",
    "    dashboard_freq_results[var] = freq_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-square tests: C vs D for each dashboard variable\n",
    "print(\"\\nCHI-SQUARE TESTS: CONDITION C vs D\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "chi_results = []\n",
    "\n",
    "for var in available_vars:\n",
    "    print(f\"\\n--- {var} ---\")\n",
    "    \n",
    "    # Only test if both conditions have variation\n",
    "    n_categories_c = df_c1[df_c1['condition'] == 'C'][var].nunique()\n",
    "    n_categories_d = df_c1[df_c1['condition'] == 'D'][var].nunique()\n",
    "    \n",
    "    if n_categories_c > 1 and n_categories_d > 1:\n",
    "        result = chi_square_test(df_c1, 'condition', var)\n",
    "        \n",
    "        print(f\"χ²({result['df']}) = {result['chi2']:.3f}, p = {result['p']:.4f}\")\n",
    "        print(f\"Cramér's V = {result['cramers_v']:.3f} ({result['v_interpretation']})\")\n",
    "        print(f\"Significant: {'Yes' if result['p'] < ALPHA else 'No'}\")\n",
    "        \n",
    "        chi_results.append({\n",
    "            'Variable': var,\n",
    "            'χ²': round(result['chi2'], 3),\n",
    "            'df': result['df'],\n",
    "            'p': round(result['p'], 4),\n",
    "            'Cramér\\'s V': round(result['cramers_v'], 3),\n",
    "            'Interpretation': result['v_interpretation'],\n",
    "            'Significant': 'Yes' if result['p'] < ALPHA else 'No'\n",
    "        })\n",
    "    else:\n",
    "        print(\"Insufficient variation for chi-square test.\")\n",
    "        chi_results.append({\n",
    "            'Variable': var,\n",
    "            'χ²': 'N/A',\n",
    "            'df': 'N/A',\n",
    "            'p': 'N/A',\n",
    "            'Cramér\\'s V': 'N/A',\n",
    "            'Interpretation': 'N/A',\n",
    "            'Significant': 'N/A'\n",
    "        })\n",
    "\n",
    "chi_results_df = pd.DataFrame(chi_results)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Summary:\")\n",
    "chi_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top configuration profiles\n",
    "print(\"\\nTOP DASHBOARD CONFIGURATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if len(available_vars) == 4:\n",
    "    # Create configuration string\n",
    "    df_c1['config'] = (df_c1['dashboard_scope'].astype(str) + ' | ' +\n",
    "                       df_c1['dashboard_purpose'].astype(str) + ' | ' +\n",
    "                       df_c1['dashboard_storage'].astype(str) + ' | ' +\n",
    "                       df_c1['dashboard_retention'].astype(str))\n",
    "    \n",
    "    # Top 10 configurations\n",
    "    top_configs = df_c1['config'].value_counts().head(10)\n",
    "    print(\"\\nTop 10 most common configurations (scope | purpose | storage | retention):\")\n",
    "    for i, (config_str, count) in enumerate(top_configs.items(), 1):\n",
    "        pct = count / len(df_c1) * 100\n",
    "        print(f\"{i}. {config_str}: n={count} ({pct:.1f}%)\")\n",
    "else:\n",
    "    print(\"Not all 4 dashboard variables available for configuration analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dashboard visualizations\n",
    "if len(available_vars) >= 2:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    colors_cd = {'C': '#45B7D1', 'D': '#96CEB4'}\n",
    "    \n",
    "    for i, var in enumerate(available_vars[:4]):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Get data\n",
    "        data_c = df_c1[df_c1['condition'] == 'C'][var].value_counts(normalize=True) * 100\n",
    "        data_d = df_c1[df_c1['condition'] == 'D'][var].value_counts(normalize=True) * 100\n",
    "        \n",
    "        # Combine for plotting\n",
    "        categories = sorted(set(data_c.index) | set(data_d.index))\n",
    "        x = np.arange(len(categories))\n",
    "        width = 0.35\n",
    "        \n",
    "        vals_c = [data_c.get(cat, 0) for cat in categories]\n",
    "        vals_d = [data_d.get(cat, 0) for cat in categories]\n",
    "        \n",
    "        bars1 = ax.bar(x - width/2, vals_c, width, label='Condition C', color=colors_cd['C'], alpha=0.8)\n",
    "        bars2 = ax.bar(x + width/2, vals_d, width, label='Condition D', color=colors_cd['D'], alpha=0.8)\n",
    "        \n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels([str(c)[:15] for c in categories], rotation=45, ha='right', fontsize=9)\n",
    "        ax.set_ylabel('Percentage (%)', fontsize=10)\n",
    "        ax.set_title(var.replace('dashboard_', '').title(), fontsize=11, fontweight='bold')\n",
    "        ax.legend(fontsize=9)\n",
    "        ax.set_ylim(0, 100)\n",
    "    \n",
    "    plt.suptitle('Dashboard Selections: Condition C vs D', fontsize=12, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Insufficient dashboard variables for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 6B: Q14 Open Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"6B: Q14 OPEN TEXT ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nQ14: 'What mattered most for your data donation decision?'\")\n",
    "\n",
    "# Check if Q14 exists\n",
    "q14_col = None\n",
    "for col in ['q14', 'Q14', 'q14_text', 'open_text', 'decision_reason']:\n",
    "    if col in df_filtered.columns:\n",
    "        q14_col = col\n",
    "        break\n",
    "\n",
    "if q14_col:\n",
    "    print(f\"\\nFound Q14 in column: {q14_col}\")\n",
    "    \n",
    "    # Response rate\n",
    "    total_n = len(df_filtered)\n",
    "    non_empty = df_filtered[q14_col].dropna()\n",
    "    non_empty = non_empty[non_empty.str.strip() != '']\n",
    "    response_rate = len(non_empty) / total_n * 100\n",
    "    \n",
    "    print(f\"Response rate: {len(non_empty)}/{total_n} ({response_rate:.1f}%)\")\n",
    "    \n",
    "    # By condition\n",
    "    print(\"\\nResponse rate by condition:\")\n",
    "    for cond in ['A', 'B', 'C', 'D']:\n",
    "        cond_df = df_filtered[df_filtered['condition'] == cond]\n",
    "        cond_responses = cond_df[q14_col].dropna()\n",
    "        cond_responses = cond_responses[cond_responses.str.strip() != '']\n",
    "        cond_rate = len(cond_responses) / len(cond_df) * 100 if len(cond_df) > 0 else 0\n",
    "        print(f\"  {cond}: {len(cond_responses)}/{len(cond_df)} ({cond_rate:.1f}%)\")\n",
    "else:\n",
    "    print(\"\\nQ14 column not found in dataset.\")\n",
    "    print(\"Available columns:\", list(df_filtered.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theme codebook\n",
    "THEME_CODEBOOK = {\n",
    "    'transparency': ['transparent', 'clear', 'understand', 'know', 'information', 'explain', 'disclosure'],\n",
    "    'control': ['control', 'choice', 'choose', 'decide', 'option', 'configure', 'granular'],\n",
    "    'anonymity': ['anonymous', 'anonymity', 'identity', 'personal', 'identifiable', 'private'],\n",
    "    'risk': ['risk', 'danger', 'unsafe', 'concern', 'worry', 'afraid', 'misuse'],\n",
    "    'purpose': ['purpose', 'research', 'academic', 'science', 'commercial', 'profit'],\n",
    "    'storage': ['storage', 'store', 'server', 'switzerland', 'local', 'location', 'where'],\n",
    "    'retention': ['delete', 'retain', 'keep', 'time', 'duration', 'permanent', 'temporary'],\n",
    "    'trust': ['trust', 'believe', 'reliable', 'credible', 'honest', 'trustworthy'],\n",
    "    'civic': ['civic', 'citizen', 'democracy', 'vote', 'public', 'society', 'benefit'],\n",
    "    'general_privacy': ['privacy', 'data protection', 'gdpr', 'sensitive']\n",
    "}\n",
    "\n",
    "def code_themes(text: str, codebook: dict) -> list:\n",
    "    \"\"\"Identify themes in text based on keyword matching.\"\"\"\n",
    "    if pd.isna(text) or text.strip() == '':\n",
    "        return []\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    themes_found = []\n",
    "    \n",
    "    for theme, keywords in codebook.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword in text_lower:\n",
    "                themes_found.append(theme)\n",
    "                break  # Only count each theme once per response\n",
    "    \n",
    "    return themes_found\n",
    "\n",
    "print(\"Theme codebook defined with\", len(THEME_CODEBOOK), \"themes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code responses\n",
    "if q14_col:\n",
    "    print(\"\\nTHEME CODING RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Apply coding\n",
    "    df_filtered['themes'] = df_filtered[q14_col].apply(lambda x: code_themes(x, THEME_CODEBOOK))\n",
    "    \n",
    "    # Count themes overall\n",
    "    all_themes = []\n",
    "    for themes_list in df_filtered['themes']:\n",
    "        all_themes.extend(themes_list)\n",
    "    \n",
    "    theme_counts = Counter(all_themes)\n",
    "    \n",
    "    print(\"\\nOverall theme frequencies:\")\n",
    "    for theme, count in theme_counts.most_common():\n",
    "        pct = count / len(non_empty) * 100 if len(non_empty) > 0 else 0\n",
    "        print(f\"  {theme}: n={count} ({pct:.1f}%)\")\n",
    "else:\n",
    "    print(\"Skipping theme coding - Q14 not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theme frequencies by condition\n",
    "if q14_col:\n",
    "    print(\"\\nTHEME FREQUENCIES BY CONDITION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    theme_by_condition = {}\n",
    "    \n",
    "    for cond in ['A', 'B', 'C', 'D']:\n",
    "        cond_df = df_filtered[df_filtered['condition'] == cond]\n",
    "        cond_themes = []\n",
    "        for themes_list in cond_df['themes']:\n",
    "            cond_themes.extend(themes_list)\n",
    "        \n",
    "        theme_by_condition[cond] = Counter(cond_themes)\n",
    "    \n",
    "    # Create comparison table\n",
    "    all_theme_names = list(THEME_CODEBOOK.keys())\n",
    "    comparison_data = []\n",
    "    \n",
    "    for theme in all_theme_names:\n",
    "        row = {'Theme': theme}\n",
    "        for cond in ['A', 'B', 'C', 'D']:\n",
    "            count = theme_by_condition[cond].get(theme, 0)\n",
    "            n_cond = len(df_filtered[df_filtered['condition'] == cond])\n",
    "            pct = count / n_cond * 100 if n_cond > 0 else 0\n",
    "            row[f'{cond} (%)'] = round(pct, 1)\n",
    "        comparison_data.append(row)\n",
    "    \n",
    "    theme_comparison_df = pd.DataFrame(comparison_data)\n",
    "    theme_comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theme frequencies by donation decision\n",
    "if q14_col:\n",
    "    print(\"\\nTHEME FREQUENCIES BY DONATION DECISION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    theme_by_decision = {}\n",
    "    \n",
    "    for decision in [0, 1]:\n",
    "        decision_df = df_filtered[df_filtered['donation_decision'] == decision]\n",
    "        decision_themes = []\n",
    "        for themes_list in decision_df['themes']:\n",
    "            decision_themes.extend(themes_list)\n",
    "        \n",
    "        theme_by_decision[decision] = Counter(decision_themes)\n",
    "    \n",
    "    # Create comparison table\n",
    "    decision_comparison = []\n",
    "    \n",
    "    for theme in all_theme_names:\n",
    "        n_decline = len(df_filtered[df_filtered['donation_decision'] == 0])\n",
    "        n_donate = len(df_filtered[df_filtered['donation_decision'] == 1])\n",
    "        \n",
    "        count_decline = theme_by_decision[0].get(theme, 0)\n",
    "        count_donate = theme_by_decision[1].get(theme, 0)\n",
    "        \n",
    "        pct_decline = count_decline / n_decline * 100 if n_decline > 0 else 0\n",
    "        pct_donate = count_donate / n_donate * 100 if n_donate > 0 else 0\n",
    "        \n",
    "        decision_comparison.append({\n",
    "            'Theme': theme,\n",
    "            'Decline (%)': round(pct_decline, 1),\n",
    "            'Donate (%)': round(pct_donate, 1),\n",
    "            'Δ (pp)': round(pct_donate - pct_decline, 1)\n",
    "        })\n",
    "    \n",
    "    decision_comparison_df = pd.DataFrame(decision_comparison)\n",
    "    decision_comparison_df.sort_values('Δ (pp)', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condition contrasts\n",
    "if q14_col:\n",
    "    print(\"\\nCONDITION CONTRASTS (Theme % differences)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    contrasts = [\n",
    "        ('A', 'B', 'Effect of adding DNL (no dashboard)'),\n",
    "        ('A', 'C', 'Effect of adding Dashboard (no DNL)'),\n",
    "        ('C', 'D', 'Effect of adding DNL (with dashboard)'),\n",
    "        ('B', 'D', 'Effect of adding Dashboard (with DNL)')\n",
    "    ]\n",
    "    \n",
    "    for cond1, cond2, description in contrasts:\n",
    "        print(f\"\\n{cond1} vs {cond2}: {description}\")\n",
    "        \n",
    "        n1 = len(df_filtered[df_filtered['condition'] == cond1])\n",
    "        n2 = len(df_filtered[df_filtered['condition'] == cond2])\n",
    "        \n",
    "        differences = []\n",
    "        for theme in all_theme_names:\n",
    "            pct1 = theme_by_condition[cond1].get(theme, 0) / n1 * 100 if n1 > 0 else 0\n",
    "            pct2 = theme_by_condition[cond2].get(theme, 0) / n2 * 100 if n2 > 0 else 0\n",
    "            diff = pct2 - pct1\n",
    "            if abs(diff) >= 5:  # Only show meaningful differences\n",
    "                differences.append((theme, diff))\n",
    "        \n",
    "        if differences:\n",
    "            differences.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "            for theme, diff in differences[:5]:\n",
    "                direction = '↑' if diff > 0 else '↓'\n",
    "                print(f\"  {theme}: {direction} {abs(diff):.1f} pp\")\n",
    "        else:\n",
    "            print(\"  No meaningful differences (≥5 pp)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representative quotes\n",
    "if q14_col:\n",
    "    print(\"\\nREPRESENTATIVE QUOTES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Get non-empty responses\n",
    "    df_with_text = df_filtered[df_filtered[q14_col].notna()].copy()\n",
    "    df_with_text = df_with_text[df_with_text[q14_col].str.strip() != '']\n",
    "    \n",
    "    if len(df_with_text) > 0:\n",
    "        # Sample 5 quotes (stratified by condition if possible)\n",
    "        quotes = []\n",
    "        \n",
    "        for cond in ['A', 'B', 'C', 'D']:\n",
    "            cond_texts = df_with_text[df_with_text['condition'] == cond]\n",
    "            if len(cond_texts) > 0:\n",
    "                # Take first valid quote from this condition\n",
    "                sample_row = cond_texts.iloc[0]\n",
    "                quotes.append({\n",
    "                    'Condition': cond,\n",
    "                    'Donated': 'Yes' if sample_row['donation_decision'] == 1 else 'No',\n",
    "                    'Quote': sample_row[q14_col][:200] + ('...' if len(str(sample_row[q14_col])) > 200 else '')\n",
    "                })\n",
    "        \n",
    "        # Add one more if we have less than 5\n",
    "        if len(quotes) < 5 and len(df_with_text) > 4:\n",
    "            remaining = df_with_text[~df_with_text.index.isin([df_with_text[df_with_text['condition'] == q['Condition']].index[0] for q in quotes])]\n",
    "            if len(remaining) > 0:\n",
    "                sample_row = remaining.iloc[0]\n",
    "                quotes.append({\n",
    "                    'Condition': sample_row['condition'],\n",
    "                    'Donated': 'Yes' if sample_row['donation_decision'] == 1 else 'No',\n",
    "                    'Quote': sample_row[q14_col][:200] + ('...' if len(str(sample_row[q14_col])) > 200 else '')\n",
    "                })\n",
    "        \n",
    "        print(f\"\\n{len(quotes)} representative quotes:\")\n",
    "        for i, q in enumerate(quotes, 1):\n",
    "            print(f\"\\n{i}. [Condition {q['Condition']}, Donated: {q['Donated']}]\")\n",
    "            print(f\"   \\\"{q['Quote']}\\\"\")\n",
    "    else:\n",
    "        print(\"No text responses available for quotes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = './output/phase6'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save dashboard results\n",
    "if len(available_vars) > 0:\n",
    "    chi_results_df.to_csv(f'{output_dir}/phase6_dashboard_chi_square_{PARTICIPANT_TYPE}.csv', index=False)\n",
    "    \n",
    "    for var, freq_table in dashboard_freq_results.items():\n",
    "        freq_table.to_csv(f'{output_dir}/phase6_{var}_frequencies_{PARTICIPANT_TYPE}.csv')\n",
    "\n",
    "# Save Q14 results\n",
    "if q14_col:\n",
    "    theme_comparison_df.to_csv(f'{output_dir}/phase6_themes_by_condition_{PARTICIPANT_TYPE}.csv', index=False)\n",
    "    decision_comparison_df.to_csv(f'{output_dir}/phase6_themes_by_decision_{PARTICIPANT_TYPE}.csv', index=False)\n",
    "\n",
    "print(f\"Results saved to {output_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 6 SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "6A: DASHBOARD BEHAVIOR (Conditions C & D, n={len(df_c1)})\n",
    "  Dashboard variables analyzed: {len(available_vars)}\n",
    "  Chi-square tests (C vs D): {len(chi_results)} completed\n",
    "  Significant differences: {sum(1 for r in chi_results if r.get('Significant') == 'Yes')}\n",
    "\n",
    "6B: Q14 OPEN TEXT ANALYSIS\n",
    "  Q14 column: {q14_col if q14_col else 'Not found'}\n",
    "  Response rate: {response_rate:.1f}% ({len(non_empty)}/{total_n}) if q14_col else 'N/A'\n",
    "  Themes coded: {len(THEME_CODEBOOK)}\n",
    "  Most common theme: {theme_counts.most_common(1)[0] if theme_counts else 'N/A'}\n",
    "\n",
    "KEY FINDINGS:\n",
    "  - Dashboard preferences {'vary' if any(r.get('Significant') == 'Yes' for r in chi_results) else 'do not significantly differ'} between C and D\n",
    "  - Most cited themes relate to: {', '.join([t for t, _ in theme_counts.most_common(3)]) if theme_counts else 'N/A'}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## Phase 6 Complete\n",
    "\n",
    "The exploratory analysis is complete. Key outputs:\n",
    "\n",
    "### 6A: Dashboard Behavior\n",
    "1. Frequency tables for each dashboard variable (overall and by condition)\n",
    "2. Chi-square tests comparing C vs D distributions\n",
    "3. Top configuration profiles\n",
    "4. Visualization of dashboard selections\n",
    "\n",
    "### 6B: Q14 Open Text\n",
    "1. Theme coding using keyword-based codebook\n",
    "2. Theme frequencies by condition (A/B/C/D)\n",
    "3. Theme frequencies by donation decision\n",
    "4. Condition contrasts (A vs B, A vs C, C vs D, B vs D)\n",
    "5. Representative quotes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
